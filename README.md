# PySpark-Implementation-of-Clinical-Trial-Dataset
# Description of Setup Required
Spark context and spark configuration are essential setup for spark programming. These setups are in-built on Databricks server; hence, a 15GB storage cluster, 2core processor, running Apache Spark 3.2.1 and Scala 2.12 was created. Also, csv file extension is running on Databricks spark session. All functions were imported from pyspark.sql.functions which is necessary for the DataFrame implementation. Datasets for this assignment was downloaded from blackboard and imported into Databricks file system (dbfs). The clinical trial files are .gz files which requires extraction. Consequently, data extraction was done in the data extraction notebook, having the extracted files copied into Databricks FileStore/tables. Bokeh was installed for visualization required in the implementations. Furthermore, setup was done for AWS implementation, creating S3-bucket, directories to store external tables, and a database. Also query result location from Athena was pointed to the S3-bucket created.
